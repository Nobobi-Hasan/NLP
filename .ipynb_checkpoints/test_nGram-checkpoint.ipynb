{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be1fd63-fc51-49a4-969a-3124394178b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe52247-6387-421d-99b6-febe211c08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGrams(doc, n, isTokenized):\n",
    "    if not isTokenized:\n",
    "        doc = [token.text for token in nlp(doc)]\n",
    "\n",
    "    doc = ' '.join(doc).lower().split(' ') #convert all to lowercase\n",
    "    grams = [doc[i:i+n] for i in range(len(doc)-n+1)]\n",
    "    print (doc)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ab760d-ba3c-4382-89d6-4af44116a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'old', 'are', 'you', 'today', 'or', 'can', 'you', 'tell', 'me', 'something', 'about', 'yourself']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['how', 'old', 'are', 'you', 'today'],\n",
       " ['old', 'are', 'you', 'today', 'or'],\n",
       " ['are', 'you', 'today', 'or', 'can'],\n",
       " ['you', 'today', 'or', 'can', 'you'],\n",
       " ['today', 'or', 'can', 'you', 'tell'],\n",
       " ['or', 'can', 'you', 'tell', 'me'],\n",
       " ['can', 'you', 'tell', 'me', 'something'],\n",
       " ['you', 'tell', 'me', 'something', 'about'],\n",
       " ['tell', 'me', 'something', 'about', 'yourself']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = 'How old are you today or can you tell me something about yourself'\n",
    "# doc = ['How', 'old', 'are', 'you', 'today']\n",
    "n = 5\n",
    "grams = nGrams(doc, n, False)\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea9a7dd-c922-4c95-8b8a-27aa6264af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e96fba-b420-48d7-b1b2-b9f873e3f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0)) #eg. {x: {y: 0}}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a3b65c-a34b-4520-b04d-0aaf7e6c4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCount(nGram, model):\n",
    "    w_1_to_n_minus_1 = tuple(nGram[:1])\n",
    "    w_n = nGram[-1]\n",
    "    model[w_1_to_n_minus_1][w_n] += 1 #eg. {w_1_to_n_minus_1: {w_n: 1}}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cc78fc-5e3b-4ba8-92d8-2bdea8606b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeProbability(model):\n",
    "    for w_1_to_n_minus_1 in model:\n",
    "        totalCount = float(sum(model[w_1_to_n_minus_1].values()))\n",
    "        for w_n in model[w_1_to_n_minus_1]:\n",
    "            model[w_1_to_n_minus_1][w_n] /= totalCount\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05e3bfe-b53e-4fd7-a762-c9c0622041e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill #extention of pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fcf7ae-4a65-4b9d-bd0c-4e5ed2f467cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, fileName):\n",
    "    with open(fileName, 'wb') as f:\n",
    "        dill.dump(model, f)\n",
    "\n",
    "def loadModel(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b2e366-51a4-4da5-99ea-5d99d350fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk #For the 1st time only\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9efa28fe-7ec9-4595-9111-5024513fe33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups as getData\n",
    "# nltk.download('reuters') #For the 1st time only\n",
    "from nltk.corpus import reuters as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e89479-c183-4556-9c1f-2b2d42459820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets._twenty_newsgroups.fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False, n_retries=3, delay=1.0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8620a76b-cf2d-420f-90bd-18e791058c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = getData(subset='train',remove=('headers','footers','qoutes'),return_X_y=True) #didn'swork directly like the Udemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be4d8c7f-a344-42c4-95e6-d80849d58ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# import urllib.request\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ssl._create_unverified_context()))\n",
    "# urllib.request.install_opener(opener)\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "X, y = getData(subset='train', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29e564a-0ab6-41cd-8fe4-434b198b9fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ffadc1-73c4-4d62-aa7c-7fb9a057a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0e7d89-054d-4417-8078-992e26bb5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "model = buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0271cf0-383c-48ac-8295-03f6d836297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in X:\n",
    "#     for nGram in nGrams(doc, n, False):\n",
    "#         model = updateCount(nGram, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eb605b9-5feb-426e-8b3f-22b5b5a0bcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No sentence tokenizer for this corpus",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\reader\\plaintext.py:89\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.sents\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Shatin/nltk_data'\n    - 'C:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Shatin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# nltk.download('reuters')\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m nGram \u001b[38;5;129;01min\u001b[39;00m nGrams(sentence, n, \u001b[38;5;28;01mTrue\u001b[39;00m):  \u001b[38;5;66;03m#Data is tokenized here. So True\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         model \u001b[38;5;241m=\u001b[39m updateCount(nGram, model)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\reader\\api.py:409\u001b[0m, in \u001b[0;36mCategorizedCorpusReader.sents\u001b[1;34m(self, fileids, categories)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msents\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\reader\\plaintext.py:91\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.sents\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m PunktTokenizer()\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence tokenizer for this corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(\n\u001b[0;32m     94\u001b[0m     [\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCorpusView(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_sent_block, encoding\u001b[38;5;241m=\u001b[39menc)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (path, enc, fileid) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabspaths(fileids, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     97\u001b[0m     ]\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: No sentence tokenizer for this corpus"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('reuters')\n",
    "for sentence in corpus.sents():\n",
    "    for nGram in nGrams(sentence, n, True):  #Data is tokenized here. So True\n",
    "        model = updateCount(nGram, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096f174-a2ae-48b9-a5fd-fd07f3eeb6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
